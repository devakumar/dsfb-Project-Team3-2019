{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSFB 2019 Project - Team 3: Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team members:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS\n",
    "\n",
    "Here are a bunch of libraries that you may need to use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import math  \n",
    "import copy\n",
    "\n",
    "import pandas_profiling\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.dummy           import DummyClassifier\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.svm             import SVC, SVR\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn.tree            import DecisionTreeClassifier\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "from sklearn.ensemble        import GradientBoostingClassifier\n",
    "\n",
    "# Supporting functions from scikit-learn\n",
    "from sklearn.metrics         import confusion_matrix\n",
    "from sklearn.metrics         import roc_curve\n",
    "from sklearn.metrics         import roc_auc_score\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree            import export_graphviz\n",
    "from sklearn.decomposition   import PCA\n",
    "\n",
    "# for text processing\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore, LsiModel, HdpModel\n",
    "\n",
    "# ignore some warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a seed for replication\n",
    "SEED = 1  # Use this anywhere a stochastic function allows you to set a seed\n",
    "\n",
    "# Additional imports\n",
    "import string\n",
    "import missingno as msno\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from gensim.models import LdaModel, LdaMulticore, LsiModel, HdpModel\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, BaggingClassifier, BaggingRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Special Plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes=[0,1], normalize=False, title='Confusion Matrix', cmap=plt.cm.Reds):\n",
    "    \"\"\" \n",
    "    Function to plot a sklearn confusion matrix, showing number of cases per prediction condition. \n",
    "    The cm parameter can be an object created by the sklearn confusion_matrix() function.\n",
    "    \n",
    "    Args:\n",
    "        cm         this must be a sklearn confusion matrix \n",
    "        classes    levels of the class being predicted; default to binary outcome\n",
    "        normalize  apply normalization by setting `normalize=True`\n",
    "        title      title for the plot\n",
    "        cmap       color map\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round (cm[i, j],2), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "def plot_roc(fpr, tpr, title='ROC Curve', note=''):\n",
    "    \"\"\"\n",
    "    Function to plot an ROC curve in a consistent way.\n",
    "    The fpr and tpr parameters can be created by the sklearn roc_curve() function.\n",
    "    \n",
    "    Args:\n",
    "        fpr        False Positive Rate (list of multiple points)\n",
    "        tpr        True Positive Rate (list of multiple points)\n",
    "        title      Title above the plot\n",
    "        note       Note to display in the bottom-right of the plot\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title(title)\n",
    "    if note: plt.text(0.6, 0.2, note)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def print_feature_importance(tree_model, feature_names):\n",
    "    \"\"\"\n",
    "    Function to print a list of features from an sklearn tree model (ranked by importance of the feature)\n",
    "    \n",
    "    Args:\n",
    "        tree_model       A sklearn DecisionTreeClassifier()\n",
    "        feature_names    A list of features used by the DecisionTreeClassifier\n",
    "    \"\"\"\n",
    "    print('Feature'.center(12), '   ',  'Importance')\n",
    "    print('=' * 30)\n",
    "    for index in reversed(np.argsort(tree_model.feature_importances_)):\n",
    "        print(str(feature_names[index]).center(12) , '   ', '{0:.4f}'.format(tree_model.feature_importances_[index]).center(8)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, alpha='auto', eta='auto', random_state=SEED)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append((coherencemodel.get_coherence(), num_topics))\n",
    "        print(f\"Number of topics : {num_topics}, Coherence score : {coherence_values[-1][0]:.3f}\")\n",
    "\n",
    "    return model_list, coherence_values        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curve(train_scores, cv_scores, x_data, y_label='', x_label='', x_logscale=False):\n",
    "    \n",
    "    plt.title('Validation Curve')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    \n",
    "    train_scores_mean = train_scores#np.mean(train_scores, axis=1)\n",
    "    cv_scores_mean    = cv_scores#np.mean(cv_scores,    axis=1)\n",
    "    \n",
    "    if x_logscale:\n",
    "        plt.semilogx(x_data, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.semilogx(x_data, cv_scores_mean,    'o-', color=\"g\",  label=\"Cross-validation score\")        \n",
    "    else:\n",
    "        plt.plot(x_data, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(x_data, cv_scores_mean,    'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        \n",
    "    plt.legend(loc='center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_train_test_function(random_model, X_P1, y_P1)\n",
    "def custom_train_test_function(model, X, y, name=\"model\", test_size=0.2, proba=False, plot_train_score=False, test_req=True, custom_test=False):\n",
    "    # Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,  random_state=SEED)\n",
    "    \n",
    "    print(f\"{'='*15}... Training the {name}...{'='*15}\")\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Print model score and best parameters\n",
    "    print(\"\\nBest parameter (CV score =%0.3f):\" % model.best_score_)\n",
    "    print(model.best_params_)\n",
    "    \n",
    "    # Plot a validation curve across levels of C, using a logscale for the x axis\n",
    "    if plot_train_score:\n",
    "        print(f\"\\n{'='*15}Training results with cross validation...{'='*15}\\n\")\n",
    "        # Print train and test scores\n",
    "        search_results = pd.DataFrame(model.cv_results_).filter(regex=(\"(mean_train_score|mean_test_score|rank_|std_train_score|std_test_score)\"))\n",
    "        display(search_results)\n",
    "    \n",
    "        plot_validation_curve(search_results.mean_train_score.values, search_results.mean_test_score.values, search_results.index.values, x_logscale=False)\n",
    "    \n",
    "    #if test_req:\n",
    "    #    custom_test_function(model, X_test, y_test, proba=proba, name=name, custom_test=custom_test)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_test_function(model, X_test, y_test, proba=False, name=\"model\", custom_test=False):\n",
    "    # Test the model and print relevent plots\n",
    "    score = 0\n",
    "    if custom_test:\n",
    "        # For linear regression (Where confusion matrix is not possible directly)\n",
    "        y_hat_ols_prob = model.predict(X_test)\n",
    "        results = []\n",
    "        for i in range(1, 100):\n",
    "            threshold = 0.01 * i\n",
    "            y_hats   = [int(v >= threshold) for v in y_hat_ols_prob]\n",
    "            correct  = [int(r[0]==r[1]) for r in zip(y_test, y_hats)]\n",
    "            accuracy = sum(correct)/len(correct)\n",
    "            results.append( (accuracy, threshold) )\n",
    "        optimal_p = sorted(results, reverse=True)[0][1]\n",
    "        print('Optimal probability threshold based on accuracy = %2.4f' % optimal_p, 'and the corresponding accuracy is ', sorted(results, reverse=True)[0][0])\n",
    "        y,x = zip(*results)\n",
    "        plt.plot(x, y)\n",
    "        plt.vlines(optimal_p, ymin=0, ymax=1, colors=['red'])\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Probability Threshold')\n",
    "        plt.show()\n",
    "        \n",
    "        fpr_ols, tpr_ols, _ = roc_curve(y_test, y_hat_ols_prob)\n",
    "        auc_ols = roc_auc_score(y_test, y_hat_ols_prob)\n",
    "        plot_roc(fpr_ols, tpr_ols, f'ROC Curve for {name}')\n",
    "        score = auc_ols\n",
    "    else :\n",
    "        if proba:\n",
    "            y_hat = model.predict_proba(X_test)[:, 1]\n",
    "        else :\n",
    "            y_hat = model.predict(X_test)\n",
    "\n",
    "            print(f\"\\n{'='*15}Confusion matrix...{'='*15}\\n\")\n",
    "            confusion_mat = confusion_matrix(y_test, y_hat)  \n",
    "            plot_confusion_matrix(confusion_mat)\n",
    "            plt.show()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            accuracy = 1 - ((confusion_mat[0][1] + confusion_mat[1][0])/(confusion_mat[0][1] + confusion_mat[1][0] + confusion_mat[1][1] + confusion_mat[0][0]))\n",
    "            print('Accuracy = ', \"{0:.4f}\".format(accuracy))\n",
    "\n",
    "        print(f\"\\n{'='*15}ROC curve on the Test data ...{'='*15}\\n\")\n",
    "        #plot roc curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_hat)\n",
    "        best_auc = roc_auc_score(y_test, y_hat)\n",
    "        plot_roc(fpr, tpr, f'ROC Curve for {name}', 'AUC = %2.4f' % best_auc)\n",
    "        plt.show()\n",
    "        score = best_auc\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scoring_metric_P7P8(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i, y in enumerate(y_true):\n",
    "        if abs(y-1) < 1e-6:\n",
    "            loss = loss + (1 - y_pred[i])*(1 - y_pred[i])\n",
    "        else:\n",
    "            loss = loss + (y_pred[i])*(y_pred[i])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scoring_metric_P9(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i, y in enumerate(y_true.values()):\n",
    "        if abs(y-1) < 1e-6:\n",
    "            loss = loss + y_pred[i]\n",
    "        else:\n",
    "            loss = loss + 2*(1 - y_pred[i])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn=pd.read_excel(\"IPO_data_to_learn.xlsx\")\n",
    "df_predict=pd.read_excel(\"IPO_data_to_predict.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**: We notice that the first column in the data has unique ID, not an useful feature for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df_learn[df_learn.columns[0]].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the data with first column as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn=pd.read_excel(\"IPO_data_to_learn.xlsx\", index_col=0)\n",
    "df_predict=pd.read_excel(\"IPO_data_to_predict.xlsx\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick look at the data and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: We observe that there 46 columns out of which 38 are numeric/bool and 8 are text type. We also notice that there are some missing data in many fields. \n",
    "\n",
    "Now we look at the detailed profile of the dataFrame using pandas profiling. (The report is provided in HTML along with the submission. Observations related to the report are given below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#file.profile_report()\n",
    "#profile = pandas_profiling.ProfileReport(df_learn)\n",
    "#profile.to_file(outputfile='learn_data_profile_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from the profile report**\n",
    "\n",
    "* Presence of missing values (Either drop or process the missig fileds)\n",
    "* High correlation among 5 fields (May be ignored or Use PCA to reduce the dimentionality)\n",
    "* Different scales, ranging from 0 to 1e9. (Need to normalize the data)\n",
    "* Missing outcome: offerPrice(3.5%) and closeDay1(3.5%). Since there is no outcome, it may not be useful to use this data, may be dropped.\n",
    "* Only 22% of the companies are marked emerging growth companies. (Possibility of bias?)\n",
    "* Most of the companies are listed in NASDAQ(2368), followed by NYSE(895)\n",
    "* Data is present from 1996 to 2018 (More data in the late 90s, but data is well spread across years)\n",
    "* Five fields that are skewed (totalProceeds, InvestmentReceived, commonEquity1, totalRevenue, nPatents)\n",
    "* 19 fileds out of 47 have missing entries. Highest missing entries in investmentReceived(45%) followed by nExecutives, priorFinancing, nVCs, patRatio, managementFee(32.9%) in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile report for the prediction data\n",
    "#profile = pandas_profiling.ProfileReport(df_predict)\n",
    "#profile.to_file(outputfile='predict_data_profile_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = copy.copy(df_learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do exploratory analysis of the data and missing fields in the cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data dristribution across various fields is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** : Write what the following cell does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete = data.columns[data.isnull().any()].tolist()\n",
    "msno.heatmap(data[incomplete], figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The following cell shows the correlation among the datafields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot correlation matrix\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(data.corr(), fignum=f.number)\n",
    "plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: We notice that most of the fields are weakly correlated. However, there are few features that has strong positive and negative correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the Close day 1\n",
    "data.boxplot(column='closeDay1')\n",
    "plt.ylabel('Close day 1 price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn['exchange'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn['industryFF5'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn['industryFF12'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn['industryFF48'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: Will probably need to only keep 1 of these, maybe FF12?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learn = df_learn.rename(columns={\"commonEquity\":\"commonEquity1\",\"commonEquity.1\":\"commonEquity2\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: Age is the age of the firm. Should replace mising values by the mean? Are the zeros indicating new firms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_learn['age'].mean())\n",
    "print(df_learn['age'].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([w for w in df_learn['age'] if w>100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: Is it possible to have so much old companies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_learn['blueSky'].median(), df_learn['blueSky'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: Should do something with *blue sky* missing value: need to decide if really useful and, if it is, how to fill it. mean, median or mean between 1sr and 3rd quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: data cleaning and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = copy.deepcopy(df_learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1A : Replacing the null fields\n",
    "**Comment**: In the following cells, we replace or drop the null fields using appropriate logic for that perticular field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['age'].fillna(processed_data['age'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['blueSky'].fillna(processed_data['blueSky'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to drop the data corresponding to missing or negative values of 'closeDay1' as this feature is very important and so replacing missing values may lead to huge misinterpretation. Moreover it corresponds to only 3.5% of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.dropna(subset=[\"closeDay1\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** : We fill the null \"Risk factor\" field with \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.rf.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['patRatio'].fillna(processed_data['patRatio'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_mean = [\"sharesOfferedPerc\", \"investmentReceived\", \"commonEquity1\", \"managementFee\", \"commonEquity2\", \"bookValue\", \"totalAssets\", \"totalRevenue\", \"netIncome\", \"roa\", \"leverage\", \"priorFinancing\", \"ipoSize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[replace_mean].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[replace_mean] = processed_data[replace_mean].apply(lambda x: x.fillna(x.mean()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[replace_mean].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_median = [\"nExecutives\", \"nVCs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[replace_median].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[replace_median] = processed_data[replace_median].apply(lambda x: x.fillna(x.median()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[replace_median].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#data.profile_report()\n",
    "\n",
    "Referring to the profile report generated, attached as HTML along with submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some features are highly correlated, so we will use PCA to remove the redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot correlation matrix\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(processed_data.corr(), fignum=f.number)\n",
    "plt.xticks(range(processed_data.shape[1]), processed_data.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(processed_data.shape[1]), processed_data.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows that all the filed are pre-processed. Since there are no null fields. Also notice that the number of rows have reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1B: Manipulating the fields (Addition, deletion)\n",
    "\n",
    "**Comments**\n",
    "\n",
    "1. 'issuer' is unique, hence will not be a useful feature for prediction. We drop it from the list of useful features and set it as index.\n",
    "2. 'Exchange' is categorical (3 distinct values). We create dummies\n",
    "3. 'IndustryFF12' is also categorical (5 distinct values)\n",
    "4. ['manager', 'city'] have high cardinality, hence dropped from the useful features\n",
    "5. Update True/False fields to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique field 'issuer' is made index of the cell\n",
    "processed_data.set_index('issuer', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pd.get_dummies(data=processed_data, columns= ['exchange', 'industryFF12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.drop(['manager', 'city'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating additional field for return and raisingPrice(upORdown)\n",
    "processed_data['return'] = (processed_data['closeDay1'] - processed_data['offerPrice'])/(processed_data['offerPrice'] + 1e-10)\n",
    "processed_data[\"raisingPrice\"] = (processed_data[\"return\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all boolean fields to int type (0-False or 1-True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[processed_data.select_dtypes([np.bool]).columns] = processed_data.select_dtypes([np.bool]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['raisingPrice'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processed_data.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1C: Text pre-process on Risk Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English Stopwords from NLTK & Extend the stop word list\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['risk', 'factor', 'company', 'stock', 'may', 'inc', 'co', 'result', 'financial', \\\n",
    "               'portfolio', 'business', 'market', 'customer', 'service', 'price', 'management', \\\n",
    "              'product', 'operation', 'adversely', 'new', 'revenue', 'sale', 'operating', \\\n",
    "                   'adverse', 'effect', 'consider', 'careful', 'need', 'cost', 'information', 'investment', \\\n",
    "                   'fact', 'carefully', 'following', 'common'\n",
    "              ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stop_words), stop_words[:10])\n",
    "''.join(stop_words).__contains__('could')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks how many percent of risk facor are NaN values\n",
    "processed_data['rf'].isna().sum()/len(processed_data['rf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file them with a blank\n",
    "processed_data['rf'] = processed_data['rf'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to clean some given text\n",
    "def clean_re(txt):\n",
    "    txt = re.sub(f\"[{string.punctuation}]\", \"\", str(txt))\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clean_re\n",
    "txt = \"akjdf!!@#$%^&*()_/*965214\"\n",
    "clean_re(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clean_re() to all features\n",
    "processed_data['rf'] = processed_data['rf'].apply(clean_re)    \n",
    "processed_data['rf'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(txt):\n",
    "    return ' '.join([wnl.lemmatize(t.lower()) for t in txt.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatize_text() to all features  \n",
    "processed_data['rf'] = processed_data['rf'].apply(lemmatize_text)    \n",
    "processed_data['rf'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We notice tht the wordnetlimmatizer doesn't process words such as carefully -> careful, we we wondering about using other nltk tools such as ones shown below. However, the Lancaster Stemmer process words beyond recognition such as creafully-> car, so we decided to go ahead without further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lnl=nltk.stem.LancasterStemmer()\n",
    "\n",
    "#def lemmatize_text_lnl(txt):\n",
    "#    return ' '.join([lnl.stem(t.lower()) for t in txt.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatize_text() to all features  \n",
    "#processed_data['rf'] = processed_data['rf'].apply(lemmatize_text_lnl)    \n",
    "#processed_data['rf'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom function to remove stopwords\n",
    "def clean_stopwords(txt):\n",
    "    li = list(txt.lower().split(\" \"))\n",
    "    txt = [word for word in li if word not in stop_words]\n",
    "    return ' '.join(txt)\n",
    "\n",
    "#pattern = re.compile(r'(?i)\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
    "#def clean_stopwords(txt):\n",
    "#    txt = pattern.sub('', txt)\n",
    "#    return txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clean_stopwords\n",
    "txt = \" \".join(stop_words).lower()\n",
    "print(\"**All stop words**:\", txt)\n",
    "print(\"Cleaned text : \", clean_stopwords(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to remove stopwords\n",
    "processed_data['rf'] = processed_data['rf'].apply(clean_stopwords)    \n",
    "processed_data['rf'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom function to wrap simple_preprocess() from gensim\n",
    "#def sp(txt):\n",
    "#    return simple_preprocess(txt)\n",
    "\n",
    "# Define custom function to wrap simple_preprocess() from gensim\n",
    "def wrap_simple_preprocess(txt):\n",
    "    return simple_preprocess(str(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply simple_preprocess() to all features\n",
    "processed_data['rf'] = processed_data['rf'].apply(wrap_simple_preprocess)    \n",
    "processed_data['rf'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from the processed data for training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2A: Extracting features from 'rf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we utilize NLP to process risk factors and extract useful features for prediction.\n",
    "Reference: [web link](https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic models\n",
    "\n",
    "We try various topic models (LSI, HDA, LDA) and select one of them based on the coherence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(processed_data['rf'])\n",
    "corpus = [id2word.doc2bow(doc) for doc in processed_data['rf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We create a LDA model with initial number of topics, to check if it works before tuning the hyperparameter num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 9\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word, num_topics=no_topics, random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Check coherence score metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_lda = CoherenceModel(model=lda_model, texts=processed_data[\"rf\"], dictionary=id2word)\n",
    "print(f\"Coherence score : {cm_lda.get_coherence():.3f}\")\n",
    "coherence_values = [(cm_lda, no_topics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Hyper parameter selection for the LDA model based on coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_models, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=processed_data.rf, start=1, limit=15, step=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Number of topics : 2, Coherence score : 0.295\n",
    "Number of topics : 3, Coherence score : 0.293\n",
    "Number of topics : 4, Coherence score : 0.310\n",
    "Number of topics : 5, Coherence score : 0.291\n",
    "Number of topics : 6, Coherence score : 0.296\n",
    "Number of topics : 7, Coherence score : 0.302\n",
    "Number of topics : 8, Coherence score : 0.296\n",
    "Number of topics : 9, Coherence score : 0.314\n",
    "Number of topics : 10, Coherence score : 0.311\n",
    "Number of topics : 11, Coherence score : 0.299\n",
    "Number of topics : 12, Coherence score : 0.292\n",
    "Number of topics : 13, Coherence score : 0.294\n",
    "Number of topics : 14, Coherence score : 0.294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_values = np.array(coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_selected = np.argmax(coherence_values[:,0])\n",
    "no_topics_selected = coherence_values[index_selected,1]\n",
    "no_topics_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word, num_topics=no_topics_selected, alpha='auto', eta='auto', random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topics-keywords\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating other NLP models to process 'risk factor' text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = LsiModel(corpus=corpus, num_topics=no_topics_selected, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp_model = HdpModel(corpus=corpus, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = processed_data.rf\n",
    "lda_coherence = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word).get_coherence()\n",
    "lsi_coherence = CoherenceModel(model=lsi_model, texts=texts, dictionary=id2word).get_coherence()\n",
    "hdp_coherence = CoherenceModel(model=hdp_model, texts=texts, dictionary=id2word).get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coherence = pd.DataFrame({'LDA':[lda_coherence], 'LSI':[lsi_coherence], 'HDP':[hdp_coherence]}, index=['Coherence score'])\n",
    "df_coherence.T.plot.bar()\n",
    "plt.ylabel(\"Coherence score\")\n",
    "df_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** : We notice that LSI, LDA and HDP models have different coherence score. We observe that HDP has highest score. We select LDA because the number of selected topics are less and more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topics-keywords\n",
    "#pyLDAvis.enable_notebook()\n",
    "#vis = pyLDAvis.gensim.prepare(hdp_model, corpus, id2word)\n",
    "#vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting feature vector for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_rf_model = hdp_model.suggested_lda_model()\n",
    "selected_topic_model = lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = processed_data.rf.shape[0]\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_rf = []\n",
    "for i in range(rows):\n",
    "    top_topics = selected_topic_model.get_document_topics(corpus[i], minimum_probability=0.0)\n",
    "    #top_topics = hdp_model.evaluate_test_corpus([corpus[i]])\n",
    "    topic_vec = [top_topics[i][1] for i in range(int(no_topics_selected))]\n",
    "    #topic_vec.extend([len(processed_data[\"rf\"].iloc[i])])\n",
    "    #topic_vec.extend([len(''.join(processed_data[\"rf\"].iloc[i]))]) # length review\n",
    "    features_rf.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_features = pd.DataFrame(features_rf, columns=range(int(no_topics_selected)), index=processed_data.rf.index).add_prefix('rf_')\n",
    "pd.concat([df_rf_features, df_rf_features.sum(axis=1)], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsi_model = LsiModel(corpus=corpus, num_topics=25, id2word=id2word)\n",
    "#lsi_model.show_topics(num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsi_topics = lsi_model.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2B: Feature selection from processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features and targets from the processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select targets \n",
    "y_data_labels = ['closeDay1', 'return', 'raisingPrice']\n",
    "Y_data = processed_data[y_data_labels]\n",
    "\n",
    "X_data = processed_data.drop(y_data_labels, axis=1)\n",
    "Y_data.columns, X_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the text fields 'rf', 'industryFF12', 'industryFF48' and replace them the extracted features using text analysis for 'rf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.drop(['rf', 'industryFF5', 'industryFF48'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_data, df_rf_features], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection for various predictors (P1 o P9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Select features for Prediction 1 and look at the target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data has all features except 'rf'\n",
    "X_P1, y_P1 = X_data, Y_data.raisingPrice\n",
    "assert(X_P1.shape[0] == y_P1.shape[0])\n",
    "y_P1.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Select features for Prediction 2 and look at the target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame X contains all numeric features including extracted features from 'rf'\n",
    "X_P2, y_P2 = X.filter(regex=(\"(rf_*)|(FF12_*)|(year)\")), Y_data.raisingPrice\n",
    "assert(X_P2.shape[0] == y_P2.shape[0])\n",
    "y_P2.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Select features for Prediction 3 and look at the target variable distribution. We select all the available features for predictions from P3 to P9, reduce the dimention using PCA so that features that are not relevent are filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_P3 = Y_data.raisingPrice\n",
    "assert(X.shape[0] == y_P3.shape[0])\n",
    "X.shape, y_P3.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Target variable 'return' distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data['return'].reset_index().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Select features for Prediction 4 and look at the target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_P4 = (Y_data['return'] > 0.20).astype('int')\n",
    "assert(X.shape[0] == y_P4.shape[0])\n",
    "X.shape, y_P4.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Select features for Prediction 5 and look at the target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_P5 = (Y_data['return'] < -0.20).astype('int')\n",
    "assert(X.shape[0] == y_P5.shape[0])\n",
    "X.shape, y_P5.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Select features for Prediction 6 and look at the target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_P6 = Y_data['closeDay1']\n",
    "assert(X.shape[0] == y_P6.shape[0])\n",
    "y_P6.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Select features for Prediction 7 and look at the target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_P7 = Y_data.raisingPrice\n",
    "assert(X.shape[0] == y_P7.shape[0])\n",
    "y_P7.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Select features for Prediction 8 and look at the target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_P8 = Y_data.raisingPrice\n",
    "assert(X.shape[0] == y_P8.shape[0])\n",
    "y_P8.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Select features for Prediction 9 and look at the target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_P9 = (Y_data['return'] < -0.10).astype('int')\n",
    "assert(X.shape[0] == y_P9.shape[0])\n",
    "y_P9.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce features using PCA, select numeric fields for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for PCA\n",
    "data_pca = X.select_dtypes([np.number])\n",
    "max_components = len(data_pca.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(data_pca, Y_data.raisingPrice, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore and print out results for the relationship between AUC and the number of PCA Component Features\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "pca = PCA()\n",
    "estimators.append(('pca', pca))\n",
    "estimators.append(('model', LogisticRegression()))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "results = []\n",
    "for c in range(1,max_components):\n",
    "    pipeline.set_params(pca__n_components=c)\n",
    "    pipeline.fit(X_train,y_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_test)       # use validation set during hyper-parameter tuning\n",
    "    auc_lml1 = roc_auc_score(y_test, y_train_pred[:,1])   \n",
    "    results.append( (auc_lml1, c)  )\n",
    "\n",
    "df_auc_vs_pca = pd.DataFrame(results, columns=['AUC', 'n_components'])\n",
    "\n",
    "df_auc_vs_pca.plot('n_components', 'AUC')\n",
    "plt.savefig('pca_n_components.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pd.DataFrame(np.cumsum(pca.explained_variance_ratio_), index=range(1,max_components), columns=['Cummulative explained variance'])\n",
    "explained_variance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for optimum PCA components suing the cross validation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'pca__n_components': range(1, max_components),\n",
    "    'model__C': range(1, 10),\n",
    "}\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=False, random_state=SEED)\n",
    "search = GridSearchCV(pipeline, param_grid, iid=False, cv=cv)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = search.best_estimator_.get_params()['pca']\n",
    "pca.n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA spectrum\n",
    "pca.fit(X_train)\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance')\n",
    "\n",
    "ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "\n",
    "ax1.plot(df_auc_vs_pca.AUC, linewidth=2)\n",
    "ax1.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "plt.savefig('pca_explained_variance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Training, tuning, and testing of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in training the model and selection of best hyper parameters\n",
    "0. Spit the dataset in to traing and testing sets\n",
    "1. Create pipeline for the respective model\n",
    "2. Select parameters grid for the respective model\n",
    "3. Create cross validation scheme\n",
    "4. Search for the best estimator using Grid search using the train dataset\n",
    "5. Find the best estimator for given feature and target set using the train dataset\n",
    "6. Test the best estimator performance using test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual cross check of the best estimator**: We evaluate the model obtained from gridSearch manually and verify the performance before using it for the predictions in Part-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metric = 'roc_auc'\n",
    "scaling_metric = StandardScaler()\n",
    "pca_final = pca\n",
    "n_cv_splits = 3\n",
    "scoring_metric_regression = 'r2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random guessing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', scaling_metric))         # standardize features\n",
    "estimators.append(('model', DummyClassifier(strategy='constant', constant=np.random.choice([0, 1]), random_state=SEED)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {}\n",
    "\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=n_cv_splits, shuffle=False, random_state=SEED)\n",
    "\n",
    "# GridSearch\n",
    "# Scoring options : refer - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "random_model = GridSearchCV(pipeline, param_grid, scoring=scoring_metric, iid=False, cv=cv, n_jobs=-1, refit=True, verbose=2, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model_final = custom_train_test_function(random_model, X_P1, y_P1, name=\"Random model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps involved in training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_data.raisingPrice, test_size=0.2,  random_state=SEED)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_random = [np.random.choice([0, 1]) for v in y_test]\n",
    "pd.Series(y_hat_random).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model (AUC, ROC....etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_random = confusion_matrix(y_test, y_hat_random)  \n",
    "plot_confusion_matrix(cm_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_random = 1 - ((cm_random[0][1] + cm_random[1][0])/(cm_random[0][1] + cm_random[1][0] + cm_random[1][1] + cm_random[0][0]))\n",
    "print('Accuracy   =', \"{0:.4f}\".format(accuracy_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curve\n",
    "fpr_logit_random, tpr_logit_random, _ = roc_curve(y_test, y_hat_random)\n",
    "best_auc_random = roc_auc_score(y_test, y_hat_random)\n",
    "plot_roc(fpr_logit_random, tpr_logit_random, 'ROC Curve for random Model', 'AUC = %2.4f' % best_auc_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', scaling_metric))         # tell it to standardize features\n",
    "estimators.append(('model', DummyClassifier(strategy='most_frequent', random_state=SEED)))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {}\n",
    "\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=n_cv_splits, shuffle=False, random_state=SEED)\n",
    "\n",
    "# GridSearch\n",
    "# Scoring options : refer - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "baseline_model = GridSearchCV(pipeline, param_grid, scoring=scoring_metric, iid=False, cv=cv, n_jobs=-1, refit=True, verbose=2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_final = custom_train_test_function(baseline_model, X_P1, y_P1, name=\"Baseline model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_data.raisingPrice, test_size=0.2,  random_state=SEED)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the most commonly predicted outcome\n",
    "most_frequent_outcome = Y_data['raisingPrice'].value_counts().idxmax()\n",
    "print('Most frequent outcome =', most_frequent_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_baseline = [most_frequent_outcome ]*len(y_test)\n",
    "pd.Series(y_hat_baseline).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model (AUC, ROC....etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_baseline = confusion_matrix(y_test, y_hat_baseline)  \n",
    "plot_confusion_matrix(cm_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_baseline = 1 - ((cm_baseline[0][1] + cm_baseline[1][0])/(cm_baseline[0][1] + cm_baseline[1][0] + cm_baseline[1][1] + cm_baseline[0][0]))\n",
    "print('Accuracy   =', \"{0:.4f}\".format(accuracy_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curve\n",
    "fpr_logit_baseline, tpr_logit_baseline, _ = roc_curve(y_test, y_hat_baseline)\n",
    "best_auc_baseline = roc_auc_score(y_test, y_hat_baseline)\n",
    "plot_roc(fpr_logit_baseline, tpr_logit_baseline, 'ROC Curve for baseline Model', 'AUC = %2.4f' % best_auc_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', scaling_metric))         # tell it to standardize features\n",
    "estimators.append(('model', LinearRegression(fit_intercept=False, n_jobs=-1)))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {}\n",
    "\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=n_cv_splits, shuffle=False, random_state=SEED)\n",
    "\n",
    "# GridSearch\n",
    "# Scoring options : refer - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "linear_model = GridSearchCV(pipeline, param_grid, scoring=scoring_metric, iid=False, cv=cv, n_jobs=-1, refit=True, verbose=2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_final = custom_train_test_function(linear_model, X_P1, y_P1, name=\"Linear model\", test_req=True, custom_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1 : Linear model without 'risk factor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data.raisingPrice, test_size=0.2,  random_state=SEED)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model_p1 = LinearRegression(normalize=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model_p1.fit(X_train, y_train)\n",
    "y_hat_ols_prob = ols_model_p1.predict(X_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(1, 100):\n",
    "    threshold = 0.01 * i\n",
    "    y_hats   = [int(v >= threshold) for v in y_hat_ols_prob]\n",
    "    correct  = [int(r[0]==r[1]) for r in zip(y_train_val, y_hats)]\n",
    "    accuracy = sum(correct)/len(correct)\n",
    "    results.append( (accuracy, threshold) )\n",
    "optimal_p = sorted(results, reverse=True)[0][1]\n",
    "print('Optimal probability threshold based on accuracy = %2.4f' % optimal_p, 'and the corresponding accuracy is ', sorted(results, reverse=True)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = zip(*results)\n",
    "plt.plot(x, y)\n",
    "plt.vlines(optimal_p, ymin=0, ymax=1, colors=['red'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Probability Threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model (AUC, ROC....etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_ols, tpr_ols, _ = roc_curve(y_train_val, y_hat_ols_prob)\n",
    "auc_ols = roc_auc_score(y_train_val, y_hat_ols_prob)\n",
    "plot_roc(fpr_ols, tpr_ols, 'ROC Curve for Linear Probability Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_ols_prob_test = ols_model_p1.predict(X_test)\n",
    "y_hats   = [int(v >= optimal_p) for v in y_hat_ols_prob_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_ols = roc_auc_score(y_test, y_hats)\n",
    "print('AUC for linear probability model = %2.4f' % auc_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_baseline = confusion_matrix(y_test, y_hats)  \n",
    "plot_confusion_matrix(cm_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_baseline = 1 - ((cm_baseline[0][1] + cm_baseline[1][0])/(cm_baseline[0][1] + cm_baseline[1][0] + cm_baseline[1][1] + cm_baseline[0][0]))\n",
    "print('Accuracy   =', \"{0:.4f}\".format(accuracy_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curve\n",
    "fpr_logit_baseline, tpr_logit_baseline, _ = roc_curve(y_test, y_hat_ols_prob_test)\n",
    "best_auc_baseline = roc_auc_score(y_test, y_hat_ols_prob_test)\n",
    "plot_roc(fpr_logit_baseline, tpr_logit_baseline, 'ROC Curve for baseline Model', 'AUC = %2.4f' % best_auc_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is above the baseline one, as we could have expected. The accuracy is also better than the baseline one. So this model is probably to simple to get very good results but is not so bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further train the linear probability model for all other predictors (P2 to P9) and select a best estimator for each accordingly. We decided to do this using a custom train_test_function_declared in the beginning. The best fit model for Prediction P2 is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_final_P2 = custom_train_test_function(linear_model, X_P2, y_P2, name=\"Linear model\", test_req=False, custom_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Logit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', scaling_metric))         # tell it to standardize features\n",
    "estimators.append(('model', LogisticRegression(penalty='l1', n_jobs=-1)))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {\n",
    "    'model__C' : np.logspace(-4, 5, 10),\n",
    "}\n",
    "\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=n_cv_splits, shuffle=False, random_state=SEED)\n",
    "\n",
    "# GridSearch\n",
    "# Scoring options : refer - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "logit_model = GridSearchCV(pipeline, param_grid, scoring=scoring_metric, iid=False, cv=cv, n_jobs=-1, refit=True, verbose=2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model_final = custom_train_test_function(logit_model, X_P1, y_P1, name=\"Logit model\", proba=True, plot_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_data.raisingPrice, test_size=0.2,  random_state=SEED)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))         # tell it to standardize features\n",
    "estimators.append(('logit_model_l1', LogisticRegression()))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators) \n",
    "pipeline.set_params(logit_model_l1__penalty='l1') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune C  \n",
    "results = []\n",
    "for c in np.logspace(-4, 5, 10):\n",
    "    pipeline.set_params(logit_model_l1__C=c) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)       # use validation set during hyper-parameter tuning\n",
    "    auc_lml1 = roc_auc_score(y_train_val, y_train_pred[:,1])   \n",
    "    results.append( (auc_lml1, c)  )\n",
    "logit_model_l1 = pipeline.named_steps['logit_model_l1']      # capture model so we can use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results \n",
    "print('C'.center(12), '   ', 'AUC'.center(8), '\\n', '=' * 25)\n",
    "for (auc, c) in results:\n",
    "    print('{0:.4f}'.format(c).rjust(12), '   ',  '{0:.4f}'.format(auc).center(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_params(logit_model_l1__C=best_C)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_prob_logit_lasso = pipeline.predict_proba(X_train_val)\n",
    "fpr_logit_lasso, tpr_logit_lasso, _ = roc_curve(y_train_val, y_prob_logit_lasso[:, 1])\n",
    "best_auc_logit_lasso = roc_auc_score(y_train_val, y_prob_logit_lasso[:,1])\n",
    "plot_roc(fpr_logit_lasso, tpr_logit_lasso, 'ROC Curve for L1 Regularized Logit Model', 'AUC = %2.4f' % best_auc_logit_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Test final model \n",
    "pipeline.set_params(logit_model_l1__C=best_C)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_prob_logit_lasso = pipeline.predict_proba(X_test)\n",
    "fpr_logit_lasso, tpr_logit_lasso, _ = roc_curve(y_test, y_prob_logit_lasso[:, 1])\n",
    "best_auc_logit_lasso = roc_auc_score(y_test, y_prob_logit_lasso[:, 1])\n",
    "plot_roc(fpr_logit_lasso, tpr_logit_lasso, 'ROC Curve for L1 Regularized Logit Model', 'AUC = %2.4f' % best_auc_logit_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators2 = []\n",
    "estimators2.append(('standardize', StandardScaler()))         # tell it to standardize features\n",
    "estimators2.append(('ridge', Ridge()))  # tell it to use a logit model\n",
    "pipeline2 = Pipeline(estimators2) \n",
    "#pipeline2.set_params(logit_model_l2__penalty='l2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune C  \n",
    "results2 = []\n",
    "for c in np.logspace(-4, 5, 10):\n",
    "    pipeline2.set_params(ridge__alpha=c) \n",
    "    pipeline2.fit(X_train_train,y_train_train)\n",
    "    y_train_pred2 = pipeline2.predict(X_train_val)       # use validation set during hyper-parameter tuning\n",
    "    auc_lml2 = roc_auc_score(y_train_val, y_train_pred2)   \n",
    "    results2.append( (auc_lml2, c)  )\n",
    "ridge = pipeline2.named_steps['ridge']      # capture model so we can use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results \n",
    "print('C'.center(12), '   ', 'AUC'.center(8), '\\n', '=' * 25)\n",
    "for (auc, c) in results2:\n",
    "    print('{0:.4f}'.format(c).rjust(12), '   ',  '{0:.4f}'.format(auc).center(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c=0.100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Test final model \n",
    "pipeline2.set_params(ridge__alpha=best_c)\n",
    "pipeline2.fit(X_train,y_train)\n",
    "y_prob_logit_ridge = pipeline2.predict(X_test)\n",
    "fpr_logit_ridge, tpr_logit_ridge, _ = roc_curve(y_test, y_prob_logit_ridge)\n",
    "best_auc_logit_ridge = roc_auc_score(y_test, y_prob_logit_ridge)\n",
    "plot_roc(fpr_logit_ridge, tpr_logit_ridge, 'ROC Curve for L2 Regularized Logit Model', 'AUC = %2.4f' % best_auc_logit_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of logistic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model_comp = Pipeline([('s', StandardScaler()), ('m', LogisticRegression(penalty='l2', C = 1e10))]).fit(X_train_train, y_train_train).named_steps['m']\n",
    "print('REGULARIZATION'.center(20), 'NONE'.center(10), 'L1'.center(10))\n",
    "print('=' * 50)\n",
    "features = list(set(list(processed_data.columns)) - set(['raisingPrice','rf','closeDay1','offerPrice']))\n",
    "for (varname, lm_coef, lml1_coef) in zip(features, logit_model_comp.coef_[0], logit_model_l1.coef_[0]):\n",
    "    lm_coeff  = \"{0:.4f}\".format(lm_coef).rjust(10)\n",
    "    lml1_coef = \"{0:.4f}\".format(lml1_coef).rjust(10) if lml1_coef > 0.0001 else \"\"\n",
    "    \n",
    "    print(str(varname).center(20), lm_coeff, lml1_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a very high auc for the lasso model: it may be good to use it at least for P1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. KNN\n",
    "\n",
    "In this section, we build the pipeline to classify positive and negetive cases in the given data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', scaling_metric))         # tell it to standardize features\n",
    "estimators.append(('model', KNeighborsClassifier(n_jobs=-1)))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {\n",
    "    'model__n_neighbors' : range(1, 50, 5),\n",
    "}\n",
    "\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=n_cv_splits, shuffle=False, random_state=SEED)\n",
    "\n",
    "# GridSearch\n",
    "# Scoring options : refer - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "knn_model = GridSearchCV(pipeline, param_grid, scoring=scoring_metric, iid=False, cv=cv, n_jobs=-1, refit=True, verbose=2, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model_final = custom_train_test_function(knn_model, X_P1, y_P1, name=\"KNN model\", proba=True, plot_train_score=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', scaling_metric))         # tell it to standardize features\n",
    "estimators.append(('model', DecisionTreeClassifier(random_state=SEED)))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {\n",
    "    'model__max_depth' : range(1, 32, 2),\n",
    "}\n",
    "\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=n_cv_splits, shuffle=False, random_state=SEED)\n",
    "\n",
    "# GridSearch\n",
    "# Scoring options : refer - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "decision_trees_model = GridSearchCV(pipeline, param_grid, scoring=scoring_metric, iid=False, cv=cv, n_jobs=-1, refit=True, verbose=2, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_trees_model_final = custom_train_test_function(decision_trees_model, X_P1, y_P1, name=\"Decision tree model\", proba=True, plot_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_data.raisingPrice, test_size=0.2,  random_state=SEED)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "estimators = []\n",
    "estimators.append(('DTC_model', DecisionTreeClassifier()))\n",
    "pipeline = Pipeline(estimators) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune D   \n",
    "results = []\n",
    "for D in range(1,32):\n",
    "    pipeline.set_params(DTC_model__max_depth=D) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)       # use validation set during hyper-parameter tuning\n",
    "    auc_dtc = roc_auc_score(y_train_val, y_train_pred[:,1])   \n",
    "    results.append((auc_dtc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results \n",
    "df = pd.DataFrame(results)\n",
    "plt.figure(1)\n",
    "plt.plot(df[1],df[0])\n",
    "plt.xlabel('max depth')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best D\n",
    "r = np.array(results)\n",
    "best_D = r[np.argmax(r[:,0]),1]\n",
    "print ('\"best_D\" = ', best_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "y_test_pred_DT = pipeline.predict_proba(X_test) \n",
    "dtc = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "y_train_predDT = dtc.predict_proba(X_test)\n",
    "auc_DT = roc_auc_score(y_test, y_train_predDT[:,1])\n",
    "auc_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC\n",
    "fpr, tpr , _ = roc_curve(y_test, y_train_predDT[:,1])\n",
    "auc = roc_auc_score(y_test, y_train_predDT[:,1])\n",
    "plot_roc(fpr, tpr)\n",
    "print(auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', scaling_metric))         # tell it to standardize features\n",
    "estimators.append(('model', RandomForestClassifier(n_jobs=-1)))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {\n",
    "    'model__n_estimators' : range(1, 251, 10),\n",
    "}\n",
    "\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=n_cv_splits, shuffle=False, random_state=SEED)\n",
    "\n",
    "# GridSearch\n",
    "# Scoring options : refer - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "random_forests_model = GridSearchCV(pipeline, param_grid, scoring=scoring_metric, iid=False, cv=cv, n_jobs=-1, refit=True, verbose=2, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forests_model_final = custom_train_test_function(random_forests_model, X_P1, y_P1, name=\"Random forests model\", proba=True, plot_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature importance\n",
    "columns = X_P1.columns\n",
    "feat_importances = pd.Series(random_forests_model_final.best_estimator_.named_steps['model'].feature_importances_, index=columns)\n",
    "plt.figure(figsize=(10,20))\n",
    "feat_importances.nlargest(40).plot(kind='barh')\n",
    "\n",
    "print_feature_importance(random_forests_model_final.best_estimator_.named_steps['model'], feature_names= columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features \n",
    "features = list(set(list(data.columns)) - set(['raisingPrice','rf']))\n",
    "#X = data.loc[:, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_data.raisingPrice, test_size=0.2,  random_state=SEED)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('rfc_model', RandomForestClassifier()))\n",
    "pipeline = Pipeline(estimators) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune N   \n",
    "results = []\n",
    "for N in range(1,250,10):\n",
    "    pipeline.set_params(rfc_model__n_estimators=N) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_train_pred = pipeline.predict_proba(X_train_val)       # use validation set during hyper-parameter tuning\n",
    "    auc_rfc = roc_auc_score(y_train_val, y_train_pred[:,1])   \n",
    "    results.append((auc_rfc, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results \n",
    "df = pd.DataFrame(results)\n",
    "plt.figure(1)\n",
    "plt.plot(df[1],df[0])\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best N\n",
    "r = np.array(results)\n",
    "best_N = r[np.argmax(r[:,0]),1]\n",
    "print ('\"best_N\" = ', best_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final model\n",
    "pipeline.set_params(rfc_model__n_estimators=int(best_N))\n",
    "RFM = pipeline.fit(X_train,y_train)\n",
    "y_test_pred_RF = pipeline.predict_proba(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC\n",
    "\n",
    "fpr, tpr , _ = roc_curve(y_test, y_test_pred_RF[:,1])\n",
    "auc = roc_auc_score(y_test, y_test_pred_RF[:,1])\n",
    "plot_roc(fpr, tpr)\n",
    "print(auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Support Vector Machines Classifier : SVC\n",
    "\n",
    "Using numeric data and SVC classifier for predicting positive and negatve cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', scaling_metric))         # tell it to standardize features\n",
    "estimators.append(('model', SVC(probability=True, random_state=SEED)))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {\n",
    "    'model__C' : np.logspace(-2, 3, 5),\n",
    "    'model__kernel' : ('linear', 'rbf')\n",
    "}\n",
    "\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=n_cv_splits, shuffle=False, random_state=SEED)\n",
    "\n",
    "# GridSearch\n",
    "# Scoring options : refer - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "svc_model = GridSearchCV(pipeline, param_grid, scoring=scoring_metric, iid=False, cv=cv, n_jobs=-1, refit=True, verbose=2, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model_final = custom_train_test_function(svc_model, X_P1, y_P1, name=\"SVC model\", proba=True, plot_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_data.raisingPrice, test_size=0.2,  random_state=SEED)\n",
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline for features except 'rf'\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('svc', SVC(C=1, probability=True, random_state=SEED)))\n",
    "pipeline = Pipeline(estimators) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune K\n",
    "results = []\n",
    "for c in np.logspace(-4, 5, 10):\n",
    "    pipeline.set_params(svc__C=c) \n",
    "    pipeline.fit(X_train_train,y_train_train)\n",
    "    y_hat = pipeline.predict_proba(X_train_val)\n",
    "    auc = roc_auc_score(y_train_val, y_hat[:,1])\n",
    "    results.append( (auc, c) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results \n",
    "print('C'.rjust(5), '   ', 'AUC'.center(8), '\\n', '=' * 20)\n",
    "for (auc, k) in results:\n",
    "    print('{0}'.format(k).rjust(5), '   ',  '{0:.4f}'.format(auc).center(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auc_svc = pd.DataFrame(results, columns=['AUC', 'svc_C'])\n",
    "\n",
    "df_auc_svc.plot('svc_C', 'AUC', logx=True)\n",
    "plt.savefig('svc_C_numeric.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'svc__C': np.arange(1, 6, 1),\n",
    "}\n",
    "cv = ShuffleSplit(n_splits=2, test_size=0.25, random_state=SEED)\n",
    "search = GridSearchCV(pipeline, param_grid, iid=False, cv=cv)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_params(svc__C=1)\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=cv)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the final parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation score\n",
    "y_hat = pipeline.predict_proba(X_train_val)\n",
    "auc = roc_auc_score(y_train_val, y_hat[:,1])\n",
    "print(f\"AUC : {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on test set\n",
    "pipeline.fit(X_test,y_test)\n",
    "y_hat = pipeline.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, y_hat[:,1])\n",
    "print(f\"AUC : {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', scaling_metric))         # tell it to standardize features\n",
    "estimators.append(('model', SVR()))  # tell it to use a logit model\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {\n",
    "    'model__C' : np.logspace(-2, 3, 5),\n",
    "    'model__kernel' : ('linear', 'rbf')\n",
    "}\n",
    "\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=n_cv_splits, shuffle=False, random_state=SEED)\n",
    "\n",
    "# GridSearch\n",
    "# Scoring options : refer - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "svr_model = GridSearchCV(pipeline, param_grid, scoring=scoring_metric_regression, iid=False, cv=cv, n_jobs=-1, refit=True, verbose=2, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svr_model_final = custom_train_test_function(svr_model, X_P2, y_P2, name=\"SVR model\", proba=True, plot_train_score=True, test_req=True, custom_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Neural net classifier (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', scaling_metric))         # tell it to standardize features\n",
    "estimators.append(('model', MLPClassifier(solver='lbfgs', random_state=SEED)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {\n",
    "    'model__alpha' : np.logspace(-4, 5, 10),\n",
    "}\n",
    "\n",
    "# Select cross validation scheme\n",
    "cv = StratifiedKFold(n_splits=n_cv_splits, shuffle=False, random_state=SEED)\n",
    "\n",
    "# GridSearch\n",
    "# Scoring options : refer - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "cnn_model = GridSearchCV(pipeline, param_grid, scoring=scoring_metric, iid=False, cv=cv, n_jobs=-1, refit=True, verbose=2, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_final = custom_train_test_function(cnn_model, X_P1, y_P1, name=\"CNN model\", proba=True, plot_train_score=True, test_req=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble modeling is a process where multiple diverse base models are used to predict an outcome. ... Even though the ensemble model has multiple base models within the model, it acts and performs as a single model. Most of the practical data science applications utilize ensemble modeling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we try ensembles on the above trained baseline estimators and compare the performance for P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 5\n",
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_P1, y_P1, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_random       = BaggingClassifier(random_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "ensemble_baseline     = BaggingClassifier(baseline_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "ensemble_logit        = BaggingClassifier(logit_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "ensemble_linear       = BaggingRegressor(linear_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "ensemble_knn          = BaggingClassifier(knn_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "ensemble_decision_tree= BaggingClassifier(decision_trees_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "ensemble_random_forest= BaggingClassifier(random_forests_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "ensemble_svc          = BaggingClassifier(svc_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "#ensemble_svr          = BaggingRegressor(svr_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "ensemble_cnn          = BaggingClassifier(cnn_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random model\n",
    "custom_test_function(random_model_final.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Random model Test\")\n",
    "custom_test_function(ensemble_random.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Random model ensembling Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "custom_test_function(baseline_model_final.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Baseline model Test\")\n",
    "custom_test_function(ensemble_baseline.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Baseline model ensembling Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit model\n",
    "custom_test_function(logit_model_final.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Logit model Test\")\n",
    "custom_test_function(ensemble_logit.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Logit model ensembling Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model\n",
    "custom_test_function(linear_model_final.fit(X_train, y_train), X_test, y_test, proba=False, name=\"Linear model Test\", custom_test=True)\n",
    "custom_test_function(ensemble_linear.fit(X_train, y_train), X_test, y_test, proba=False, name=\"Linear model ensembling Test\", custom_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn model\n",
    "custom_test_function(knn_model_final.fit(X_train, y_train), X_test, y_test, proba=True, name=\"KNN model Test\")\n",
    "custom_test_function(ensemble_knn.fit(X_train, y_train), X_test, y_test, proba=True, name=\"KNN model ensembling Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision trees model\n",
    "custom_test_function(decision_trees_model_final.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Decision trees model Test\")\n",
    "custom_test_function(ensemble_decision_tree.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Decision trees model ensembling Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests model\n",
    "custom_test_function(random_forests_model_final.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Random forests model Test\")\n",
    "custom_test_function(ensemble_random_forest.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Random forests model ensembling Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC model\n",
    "custom_test_function(svc_model_final.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Support vector classification(SVC) model Test\")\n",
    "custom_test_function(ensemble_svc.fit(X_train, y_train), X_test, y_test, proba=True, name=\"Support vector classification(SVC) ensembling Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR model\n",
    "custom_test_function(svr_model_final.fit(X_train, y_train), X_test, y_test, proba=True, name=\"SVR model Test\", custom_test=True)\n",
    "custom_test_function(ensemble_svr.fit(X_train, y_train), X_test, y_test, proba=True, name=\"SVR model ensembling Test\", custom_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "custom_test_function(cnn_model_final.fit(X_train, y_train), X_test, y_test, proba=True, name=\"CNN model Test\")\n",
    "custom_test_function(ensemble_cnn.fit(X_train, y_train), X_test, y_test, proba=True, name=\"CNN model ensembling Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction based on the predict xls features for each of P1 to P9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each prediction, be sure to also show appropriate evaluation metrics in your Jupyter notebook. Show the standard metrics presented in class, but also the custom metrics given to you for problems 7, 8, and 9.\n",
    "\n",
    "Commit and push your predictions (along with your final jupyter notebook file) as part of your project repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict=pd.read_excel(\"IPO_data_to_predict.xlsx\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Prediction data features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the name of the collumn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = df_predict.rename(columns={\"commonEquity\":\"commonEquity1\",\"commonEquity.1\":\"commonEquity2\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing the null fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict = copy.deepcopy(df_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict['age'].fillna(processed_data_predict['age'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict['blueSky'].fillna(processed_data_predict['blueSky'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill the null \"Risk factor\" field with \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict.rf.fillna(\" \", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, one city is missing, we will replace it with a blank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict.city.fillna(\" \", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict['patRatio'].fillna(processed_data_predict['patRatio'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_mean = [\"sharesOfferedPerc\", \"investmentReceived\", \"commonEquity1\", \"managementFee\", \"commonEquity2\", \"bookValue\", \"totalAssets\", \"totalRevenue\", \"netIncome\", \"roa\", \"leverage\", \"priorFinancing\", \"ipoSize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[replace_mean].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict[replace_mean] = processed_data_predict[replace_mean].apply(lambda x: x.fillna(x.mean()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_median = [\"nExecutives\", \"nVCs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict[replace_median] = processed_data_predict[replace_median].apply(lambda x: x.fillna(x.median()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(processed_data_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the fields (Addition, deletion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'issuer' is unique, hence will not be a useful feature for prediction. We drop it from the list of useful features and set it as index.\n",
    "\n",
    "'Exchange' is categorical (3 distinct values). We create dummies\n",
    "\n",
    "'IndustryFF12' is also categorical (5 distinct values)\n",
    "\n",
    "['manager', 'city'] have high cardinality, hence dropped from the useful features\n",
    "\n",
    "Update True/False fields to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique field 'issuer' is made index of the cell\n",
    "processed_data_predict.set_index('issuer', drop=True, inplace=True)\n",
    "processed_data_predict = pd.get_dummies(data=processed_data_predict, columns= ['exchange', 'industryFF12'])\n",
    "processed_data_predict.drop(['manager', 'city'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict[processed_data_predict.select_dtypes([np.bool]).columns] = processed_data_predict.select_dtypes([np.bool]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also drop every column P*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict.drop(['P1', 'P2','P3', 'P4','P5', 'P6','P7', 'P8','P9'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_predict.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-process on Risk Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clean_re() to all features\n",
    "processed_data_predict['rf'] = processed_data_predict['rf'].apply(clean_re)    \n",
    "processed_data_predict['rf'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatize_text() to all features  \n",
    "processed_data_predict['rf'] = processed_data_predict['rf'].apply(lemmatize_text)    \n",
    "processed_data_predict['rf'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to remove stopwords\n",
    "processed_data_predict['rf'] = processed_data_predict['rf'].apply(clean_stopwords)    \n",
    "processed_data_predict['rf'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply simple_preprocess() to all features\n",
    "processed_data_predict['rf'] = processed_data_predict['rf'].apply(wrap_simple_preprocess)    \n",
    "processed_data_predict['rf'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from 'rf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_predict = corpora.Dictionary(processed_data_predict['rf'])\n",
    "corpus_predict = [id2word.doc2bow(doc) for doc in processed_data_predict['rf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting feature vector for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_predict = processed_data_predict.rf.shape[0]\n",
    "rows_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_rf = []\n",
    "for i in range(rows_predict):\n",
    "    top_topics = selected_topic_model.get_document_topics(corpus_predict[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(int(no_topics_selected))]\n",
    "    features_rf.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_features_pred = pd.DataFrame(features_rf, columns=range(int(no_topics_selected)), index=processed_data_predict.rf.index).add_prefix('rf_')\n",
    "pd.concat([df_rf_features_pred, df_rf_features_pred.sum(axis=1)], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_predict = processed_data_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We drop the text fields 'rf', 'industryFF12', 'industryFF48' and add the extracted features using text analysis\n",
    "X_data_predict.drop(['rf', 'industryFF5', 'industryFF48'], axis=1, inplace=True)\n",
    "X_predict = pd.concat([X_data_predict, df_rf_features_pred], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete = X_predict.columns[X_predict.isnull().any()].tolist()\n",
    "incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp_P1 = X_data_predict\n",
    "Xp_P2 = X_predict.filter(regex=(\"(rf_*)|(FF12_*)|(year)\"))\n",
    "# For all other predictors we use X_predict as the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(Xp_P1.shape[1] == X_P1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(Xp_P2.shape[1] == X_P2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(X_predict.shape[1] == X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom function for traing all models for a given training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_classifiers(X, y, plot_req=False, test_req=False, scoring='roc_auc', proba=False):\n",
    "    # Update scoring metric for each of the pipelines\n",
    "    random_model.scoring = scoring\n",
    "    linear_model.scoring = scoring\n",
    "    baseline_model.scoring = scoring\n",
    "    logit_model.scoring = scoring\n",
    "    knn_model.scoring = scoring\n",
    "    decision_trees_model.scoring = scoring\n",
    "    random_forests_model.scoring = scoring\n",
    "    svc_model.scoring = scoring\n",
    "    #svr_model.scoring = scoring\n",
    "    cnn_model.scoring = scoring\n",
    "    \n",
    "    # Train each of the models\n",
    "    if not proba :\n",
    "        random_model_final = custom_train_test_function(random_model, X, y, name=\"Random model\", proba=False, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "        linear_model_final = custom_train_test_function(linear_model, X, y, name=\"Linear model\", proba=False, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "        baseline_model_final = custom_train_test_function(baseline_model, X, y, name=\"Baseline model\", proba=False, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "\n",
    "    logit_model_final = custom_train_test_function(logit_model, X, y, name=\"Logit model\", proba=True, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "    knn_model_final = custom_train_test_function(knn_model, X, y, name=\"KNN model\", proba=True, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "    decision_trees_model_final = custom_train_test_function(decision_trees_model, X, y, name=\"Decision trees model\", proba=True, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "    random_forests_model_final = custom_train_test_function(random_forests_model, X, y, name=\"Random forests model\", proba=True, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "    svc_model_final = custom_train_test_function(svc_model, X, y, name=\"SVC model\", proba=True, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "    #svr_model_final = custom_train_test_function(svr_model, X, y, name=\"SVR model\", proba=True, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "    cnn_model_final = custom_train_test_function(cnn_model, X, y, name=\"CNN model\", proba=True, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "    \n",
    "    if not proba:\n",
    "        return [random_model_final, baseline_model_final, linear_model_final, logit_model_final, knn_model_final, decision_trees_model_final, random_forests_model_final, svc_model_final, cnn_model_final]\n",
    "    else :\n",
    "        return [logit_model_final, knn_model_final, decision_trees_model_final, random_forests_model_final, svc_model_final, cnn_model_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_best_regressors(X, y, plot_req=False, test_req=False):\n",
    "#    svr_model_final = custom_train_test_function(svr_model, X, y, name=\"SVR model\", proba=True, custom_test=False, test_req=test_req, plot_train_score=plot_req)\n",
    "    \n",
    "#    return [svr_model_final]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final_models(models, X_test, y_test, greater_the_better=True):\n",
    "    results = []\n",
    "    index = []\n",
    "    for model in models:\n",
    "        score = model.score(X_test, y_test)\n",
    "        name = f\"{model.best_estimator_.named_steps['model'].__str__().split('(')[0]}\"\n",
    "        if name in index:\n",
    "            name = name + '-1'\n",
    "        index.append(name)\n",
    "        #print(name, score)\n",
    "        results.append((score, model))\n",
    "\n",
    "    results = pd.DataFrame(results, index=index, columns=[scoring_metric, 'model'])#, index=range(len(P1_models)))\n",
    "    results.sort_values(by=scoring_metric, ascending=(not greater_the_better), inplace=True)\n",
    "    model_final = results.iloc[0]['model']\n",
    "    results[[scoring_metric]]\n",
    "    \n",
    "    return model_final, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_scorer_P7P8 = make_scorer(custom_scoring_metric_P7P8, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_scorer_P9 = make_scorer(custom_scoring_metric_P9, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction P1\n",
    "\n",
    "    Predict whether the closing price at the end of the first day of trading will go up (the \"positive\" case, coded as 1) or down (the \"negative\" case, coded as 0) from the offer price. You may use all data from the dataset except for the rf variable (i.e., risk factors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_P1, y_P1, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "P1_models = get_best_classifiers(X_train, y_train, scoring=scoring_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_model_final, P1_test_results = test_final_models(P1_models, X_test, y_test)\n",
    "P1_test_results[[scoring_metric]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_model_final = P1_model_final.fit(X_P1, y_P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_auc = custom_test_function(P1_model_final, X_P1, y_P1, proba=True, name=\"P1 Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best base estimator performance with bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_ensemble_model = BaggingClassifier(P1_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "P1_ensemble_model = P1_ensemble_model.fit(X_P1, y_P1)\n",
    "P1_auc_en = custom_test_function(P1_ensemble_model, X_P1, y_P1, proba=True, name=\"P1 ensemble Prediction\")\n",
    "P1_auc, P1_auc_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P1_auc_en > P1_auc:\n",
    "    print(\"Using bagging classifier for P1 ... \")\n",
    "    P1_model_final = P1_ensemble_model \n",
    "else :\n",
    "    print(\"Using the model without ensembling ...\")\n",
    "\n",
    "yhat_P1 = P1_model_final.predict(Xp_P1)\n",
    "P1 = pd.DataFrame(yhat_P1, index=df_predict.index)\n",
    "P1.hist(label=\"Prediction distribution\"), plt.legend(), plt.ylabel('Value count')\n",
    "\n",
    "df_predict.P1 = P1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction P2\n",
    "\n",
    "    Predict whether the closing price at the end of the first day of trading will go up (the \"positive\" case, coded as 1) or down (the \"negative\" case, coded as 0) from the offer price. You may use only the rf (i.e., risk factors), year, and industryFF12 variables for this prediction task. You may, however, perform additional text analysis of the rf variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_P2, y_P2, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "P2_models = get_best_classifiers(X_train, y_train, scoring=scoring_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_model_final, P2_test_results = test_final_models(P2_models, X_test, y_test)\n",
    "P2_test_results[[scoring_metric]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the final model on entire dataset and check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_model_final = P2_model_final.fit(X_P2, y_P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_auc = custom_test_function(P2_model_final, X_P2, y_P2, proba=True, name=\"P2 Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best base estimator performance with bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_ensemble_model = BaggingClassifier(P2_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "P2_ensemble_model = P2_ensemble_model.fit(X_P2, y_P2)\n",
    "P2_auc_en = custom_test_function(P2_ensemble_model, X_P2, y_P2, proba=True, name=\"P2 ensemble Prediction\")\n",
    "P2_auc, P2_auc_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P2_auc_en > P2_auc:\n",
    "    print(\"Using bagging classifier for P2 ... \")\n",
    "    P2_model_final = P2_ensemble_model \n",
    "else :\n",
    "    print(\"Using the model without ensembling ...\")\n",
    "\n",
    "yhat_P2 = P2_model_final.predict(Xp_P2)\n",
    "P2 = pd.DataFrame(yhat_P2, index=df_predict.index)\n",
    "P2.hist(label=\"Prediction distribution\"), plt.legend(), plt.ylabel('Value count')\n",
    "plt.show()\n",
    "\n",
    "df_predict.P2 = P2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all remaining problems, you may use any or all of the features.\n",
    "\n",
    "## Prediction P3\n",
    "\n",
    "    Predict whether the closing price at the end of the first day of trading will go up (the \"positive\" case, coded as 1) or down (the \"negative\" case, coded as 0) from the offer price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_P3, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "P3_models = get_best_classifiers(X_train, y_train, scoring=scoring_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3_model_final, P3_test_results = test_final_models(P3_models, X_test, y_test)\n",
    "P3_test_results[[scoring_metric]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the final model on entire dataset and check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3_model_final = P3_model_final.fit(X, y_P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3_auc = custom_test_function(P3_model_final, X, y_P3, proba=True, name=\"P3 Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best base estimator performance with bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3_ensemble_model = BaggingClassifier(P3_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "P3_ensemble_model = P3_ensemble_model.fit(X, y_P3)\n",
    "P3_auc_en = custom_test_function(P3_ensemble_model, X, y_P3, proba=True, name=\"P3 ensemble Prediction\")\n",
    "P3_auc, P3_auc_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P3_auc_en > P3_auc:\n",
    "    print(\"Using bagging classifier for P3 ... \")\n",
    "    P3_model_final = P3_ensemble_model \n",
    "else :\n",
    "    print(\"Using the model without ensembling ...\")\n",
    "\n",
    "yhat_P3 = P3_model_final.predict(X_predict)\n",
    "P3 = pd.DataFrame(yhat_P3, index=df_predict.index)\n",
    "P3.hist(label=\"Prediction distribution\"), plt.legend(), plt.ylabel('Value count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.P3 = P3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction P4\n",
    "\n",
    "    Predict whether the closing price at the end of the first day of trading will go up by more than 20% from the original offer price (the \"positive\" case, coded as 1) or not (the \"negative\" case, coded as 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_P4, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "P4_models = get_best_classifiers(X_train, y_train, scoring=scoring_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P4_model_final, P4_test_results = test_final_models(P4_models, X_test, y_test)\n",
    "P4_test_results[[scoring_metric]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the final model on entire dataset and check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P4_model_final = P4_model_final.fit(X, y_P4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P4_auc = custom_test_function(P4_model_final, X, y_P4, proba=True, name=\"P4 Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best base estimator performance with bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P4_ensemble_model = BaggingClassifier(P4_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "P4_ensemble_model = P4_ensemble_model.fit(X, y_P4)\n",
    "P4_auc_en = custom_test_function(P4_ensemble_model, X, y_P4, proba=True, name=\"P4 ensemble Prediction\")\n",
    "P4_auc, P4_auc_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P4_auc_en > P4_auc:\n",
    "    print(\"Using bagging classifier for P4 ... \")\n",
    "    P4_model_final = P4_ensemble_model \n",
    "else :\n",
    "    print(\"Using the model without ensembling ...\")\n",
    "\n",
    "yhat_P4 = P4_model_final.predict(X_predict)\n",
    "P4 = pd.DataFrame(yhat_P4, index=df_predict.index)\n",
    "P4.hist(label=\"Prediction distribution\"), plt.legend(), plt.ylabel('Value count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.P4 = P4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction P5\n",
    "\n",
    "    Predict whether the closing price at the end of the first day of trading will go down by more than 20% from the original offer price (the \"positive\" case, coded as 1) or not (the \"negative\" case, coded as 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_P5, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "P5_models = get_best_classifiers(X_train, y_train, scoring=scoring_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P5_model_final, P5_test_results = test_final_models(P5_models, X_test, y_test)\n",
    "P5_test_results[[scoring_metric]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the final model on entire dataset and check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P5_model_final = P5_model_final.fit(X, y_P5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P5_auc = custom_test_function(P5_model_final, X, y_P5, proba=True, name=\"P5 Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best base estimator performance with bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P5_ensemble_model = BaggingClassifier(P5_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "P5_ensemble_model = P5_ensemble_model.fit(X, y_P5)\n",
    "P5_auc_en = custom_test_function(P5_ensemble_model, X, y_P5, proba=True, name=\"P5 ensemble Prediction\")\n",
    "P5_auc, P5_auc_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P5_auc_en > P5_auc:\n",
    "    print(\"Using bagging classifier for P5 ... \")\n",
    "    P5_model_final = P5_ensemble_model \n",
    "else :\n",
    "    print(\"Using the model without ensembling ...\")\n",
    "\n",
    "yhat_P5 = P5_model_final.predict(X_predict)\n",
    "P5 = pd.DataFrame(yhat_P5, index=df_predict.index)\n",
    "P5.hist(label=\"Prediction distribution\"), plt.legend(), plt.ylabel('Value count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.P5 = P5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction P6\n",
    "\n",
    "    Predict the share price at the end of the first day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_P8, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "P8_models = get_best_classifiers(X_train, y_train, scoring=scoring_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the trained models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P6_model_final, P6_test_results = test_final_models(P8_models, X_test, y_test)\n",
    "P6_test_results[[scoring_metric]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrain the final model on entire dataset and check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P6_model_final = P6_model_final.fit(X, y_P6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P6_auc = custom_test_function(P6_model_final, X, y_P6, proba=True, name=\"P6 Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best base estimator performance with bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P6_ensemble_model = BaggingClassifier(P6_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "P6_ensemble_model = P6_ensemble_model.fit(X, y_P8)\n",
    "P6_auc_en = custom_test_function(P6_ensemble_model, X, y_P8, proba=True, name=\"P6 ensemble Prediction\")\n",
    "P6_auc, P6_auc_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P6_auc_en > P6_auc:\n",
    "    print(\"Using bagging classifier for P6 ... \")\n",
    "    P6_model_final = P6_ensemble_model \n",
    "else :\n",
    "    print(\"Using the model without ensembling ...\")\n",
    "\n",
    "yhat_P6 = P6_model_final.predict(X_predict)\n",
    "P6 = pd.DataFrame(yhat_P6, index=df_predict.index)\n",
    "P6.hist(label=\"Prediction distribution\"), plt.legend(), plt.ylabel('Value count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.P6 = P6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining problems, provide a predicted probability (expressed as a number from 0 to 100) that the stated event will happen.\n",
    "## Prediction P7\n",
    "\n",
    "    Predict the probability that the closing price at the end of the first day of trading will go up by more than 5% from the original offer price.\n",
    "\n",
    "Scoring Metric for P7: Your predictions will be evaluated in the following manner (where scored points are bad). For every observation, make a predicted probability, p, ranging from 0 to 100. For predictions where the event turns out to be FALSE, a score of p * p (i.e., the square of your predicted probability for that event) will be assessed. For predictions where the event turns out to be TRUE, a score of (100 - p) * (100 - p) will be assessed (i.e., the square of 100 minus your predicted probability for that event). Attempt to tune you prediction model(s) accordingly.\n",
    "\n",
    "For example: If you predict 70 for an observation that ends up being FALSE, then the score for that observation would equal 4,900 (70 * 70 = 4,900); but if you predict 70 for an observation that ends up being TRUE, then the score for that observation would equal 900 (100 - 70 = 30, and 30 * 30 = 900)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_P7, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "P7_models = get_best_classifiers(X_train, y_train, scoring=custom_scorer_P7P8, proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P7_model_final, P7_test_results = test_final_models(P7_models, X_test, y_test, greater_the_better=True)\n",
    "P7_test_results[[scoring_metric]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the final model on entire dataset and check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P7_model_final = P7_model_final.fit(X, y_P7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P7_auc = custom_test_function(P7_model_final, X, y_P7, proba=True, name=\"P7 Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best base estimator performance with bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P7_ensemble_model = BaggingClassifier(P7_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "P7_ensemble_model = P7_ensemble_model.fit(X, y_P7)\n",
    "P7_auc_en = custom_test_function(P7_ensemble_model, X, y_P7, proba=True, name=\"P7 ensemble Prediction\")\n",
    "P7_auc, P7_auc_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P7_auc_en > P7_auc:\n",
    "    print(\"Using bagging classifier for P7 ... \")\n",
    "    P7_model_final = P7_ensemble_model \n",
    "else :\n",
    "    print(\"Using the model without ensembling ...\")\n",
    "\n",
    "yhat_P7 = P7_model_final.predict(X_predict)\n",
    "P7 = pd.DataFrame(yhat_P7, index=df_predict.index)\n",
    "P7.hist(label=\"Prediction distribution\"), plt.legend(), plt.ylabel('Value count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.P7 = P7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction P8\n",
    "\n",
    "    Predict the probability that the closing price at the end of the first day of trading will go up by more than 50% from the original offer price.\n",
    "\n",
    "Scoring Metric for P8: Same scoring metric as P7 above. Attempt to tune you prediction model(s) accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_P8, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "P8_models = get_best_classifiers(X_train, y_train, scoring=custom_scorer_P7P8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the trained models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P8_model_final, P8_test_results = test_final_models(P8_models, X_test, y_test)\n",
    "P8_test_results[[scoring_metric]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrain the final model on entire dataset and check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P8_model_final = P8_model_final.fit(X, y_P8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P8_auc = custom_test_function(P8_model_final, X, y_P8, proba=True, name=\"P8 Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best base estimator performance with bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P8_ensemble_model = BaggingClassifier(P8_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "P8_ensemble_model = P8_ensemble_model.fit(X, y_P8)\n",
    "P8_auc_en = custom_test_function(P8_ensemble_model, X, y_P8, proba=True, name=\"P8 ensemble Prediction\")\n",
    "P8_auc, P8_auc_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P8_auc_en > P8_auc:\n",
    "    print(\"Using bagging classifier for P8 ... \")\n",
    "    P8_model_final = P8_ensemble_model \n",
    "else :\n",
    "    print(\"Using the model without ensembling ...\")\n",
    "\n",
    "yhat_P8 = P8_model_final.predict(X_predict)\n",
    "P8 = pd.DataFrame(yhat_P8, index=df_predict.index)\n",
    "P8.hist(label=\"Prediction distribution\"), plt.legend(), plt.ylabel('Value count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.P8 = P8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction P9\n",
    "\n",
    "    Predict the probability that the closing price at the end of the first day of trading will go down (the \"positive\" case, coded as 1) or not (coded as 0) by more than 10% from the original offer price.\n",
    "\n",
    "Scoring Metric for P9: Your predictions will be evaluated in the following manner (where scored points are bad). For every observation, make a predicted probability, p, ranging from 0 to 100. For predictions where the event turns out to be FALSE, a score equal to p will be assessed. For predictions where the event turns out to be TRUE, a score of 2 * (100 - p) will be assessed. Attempt to tune you prediction model(s) accordingly.\n",
    "\n",
    "For example: If you predict 70 for an observation that ends up being FALSE, the score for that observation would equal 70; but if you predict 70 for an observation that ends up being TRUE, then the score for that observation would equal 2 * (100 - 70) = 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_P9, test_size=0.2,  random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "P9_models = get_best_classifiers(X_train, y_train, scoring=custom_scorer_P9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P9_model_final, P9_test_results = test_final_models(P9_models, X_test, y_test)\n",
    "P9_test_results[[scoring_metric]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the final model on entire dataset and check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P9_model_final = P9_model_final.fit(X, y_P9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P9_auc = custom_test_function(P9_model_final, X, y_P9, proba=True, name=\"P9 Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best base estimator performance with bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P9_ensemble_model = BaggingClassifier(P9_model_final, n_estimators=n_estimators, n_jobs=-1, random_state=SEED)\n",
    "# Use the sklearn train_test_split() function to split data into \"train\", \"validation\", and \"test\" \n",
    "P9_ensemble_model = P9_ensemble_model.fit(X, y_P9)\n",
    "P9_auc_en = custom_test_function(P9_ensemble_model, X, y_P9, proba=True, name=\"P9 ensemble Prediction\")\n",
    "P9_auc, P9_auc_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P9_auc_en > P9_auc:\n",
    "    print(\"Using bagging classifier for P9 ... \")\n",
    "    P9_model_final = P9_ensemble_model \n",
    "else :\n",
    "    print(\"Using the model without ensembling ...\")\n",
    "\n",
    "yhat_P9 = P9_model_final.predict(X_predict)\n",
    "P9 = pd.DataFrame(yhat_P9, index=df_predict.index)\n",
    "P9.hist(label=\"Prediction distribution\"), plt.legend(), plt.ylabel('Value count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.P9 = P9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.concat([results[[scoring_metric]] for results in [P1_test_results, P2_test_results, P3_test_results, P4_test_results, P4_test_results, P6_test_results, P7_test_results, P8_test_results, P9_test_results]], axis=1).reset_index()\n",
    "scores.columns = ['Classifier', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
